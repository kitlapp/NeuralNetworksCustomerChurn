{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe3d269-d683-4e58-81e0-37c90f657a86",
   "metadata": {},
   "source": [
    "# Environment Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331853d-b5d4-4ec5-aba7-ceee59c1e4b4",
   "metadata": {},
   "source": [
    "**Environment Name:** Audiobooks    \n",
    "**Project Directory Name:** audiobooks_prj\n",
    "<div style=\"text-align: justify\">\n",
    "<strong>Original Imported Libraries and Python:</strong>  \n",
    "    \n",
    "-python=3.12.4 (from conda-forge)  \n",
    "-numpy=1.26.4 (from conda-forge)  \n",
    "-pandas=2.2.2 (from conda-forge)  \n",
    "-scikit-learn=1.5.1 (from conda-forge)  \n",
    "-matplotlib=3.9.1 (from conda-forge)  \n",
    "-tensorflow==2.17.0 (from pip)  \n",
    " \n",
    "    \n",
    "<strong>Project Date:</strong> July 2024\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140db97e-dabc-48ff-af1a-b33b365743c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Adespotos\\\\anaconda3\\\\envs\\\\Audiobooks\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.executable  # Display the path to the Python executable ensuring the correct env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c2a54-6ef1-47f9-9647-7bd9b93ef1bf",
   "metadata": {},
   "source": [
    "# Import Libraries & Read the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a0f3a5-e1cb-4139-884c-ee81422a0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For numerical operations and arrays.\t\n",
    "import pandas as pd  # For data manipulation and analysis.\t\n",
    "import matplotlib.pyplot as plt  # For basic plotting.\t\n",
    "import tensorflow as tf  # For building and training ML models.\n",
    "from sklearn.preprocessing import StandardScaler  # For creating scaler instances for standardization purposes.\n",
    "from imblearn.under_sampling import RandomUnderSampler  # For reducing the majority class number\n",
    "from sklearn.model_selection import train_test_split\n",
    "from audiobooks_scripts import create_datasets, create_model_train_eval_present_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b2b4c9-9f4a-49cd-bde9-2cc079b9cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel file to a DataFrame:\n",
    "df = pd.read_excel('Audiobooks_data.xlsx')\n",
    "\n",
    "# Drop customer ID column:\n",
    "df_dropped = df.copy().drop(columns='Customer ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b55325-8085-4662-a502-ff96c66051ea",
   "metadata": {},
   "source": [
    "# Dealing with the Imbalance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ed5c1c-4d34-466a-b01d-45c85e655ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Targets\n",
       "0    11847\n",
       "1     2237\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how the target values are separated:\n",
    "df_dropped['Targets'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98754a-bc89-4721-8981-6770dde6c771",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "From the above code it can be seen that the 15.88% of customers made a purchase again, whereas the rest of the customers didn't. We 'll proceed by undersampling the majority class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88344e91-b586-444b-bb4d-24901aefaed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Targets\n",
       "0    2237\n",
       "1    2237\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_dropped.drop(columns='Targets')  # Create features\n",
    "y = df_dropped['Targets']  # Create targets\n",
    "\n",
    "# Create an instance of RandomUnderSampler class:\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Undersample the separated data:\n",
    "x_undersampled, y_undersampled = under_sampler.fit_resample(x, y)\n",
    "\n",
    "# Convert to DataFrame:\n",
    "df_undersampled = pd.DataFrame(x_undersampled, columns=x.columns)\n",
    "df_undersampled['Targets'] = y_undersampled\n",
    "\n",
    "# Verify the undersampling:\n",
    "df_undersampled['Targets'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d718b3-f1b9-4103-a8b3-6a90b0f4a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_undersampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32e97e-66da-4546-bb16-a0508bda33e9",
   "metadata": {},
   "source": [
    "# Train, Validation and Test Splits with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b38c926-8464-4084-99ee-15f2ff2c4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and the targets from the previous DataFrame:\n",
    "X = df_final.drop(columns='Targets')\n",
    "y = df_final['Targets']\n",
    "\n",
    "# Assign size percentages to variables to automate processes and avoid mistakes:\n",
    "test_perc = 0.09\n",
    "mask_perc = 1 - test_perc\n",
    "val_perc = test_perc / mask_perc\n",
    "\n",
    "# Split into training+validation (mask set) and test sets:\n",
    "X_mask, X_test, y_mask, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=test_perc, \n",
    "    stratify=y,  # Ensure the new set is balanced\n",
    "    random_state=42)\n",
    "\n",
    "# Split the training+validation (mask) set into training and validation sets:\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_mask, \n",
    "    y_mask, \n",
    "    test_size=val_perc, \n",
    "    stratify=y_mask,  # Ensure the new set is balanced\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b968a9-8421-4bf2-bbaf-84e45f49f5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets\n",
      "1    1834\n",
      "0    1834\n",
      "Name: count, dtype: int64\n",
      "Targets\n",
      "1    202\n",
      "0    201\n",
      "Name: count, dtype: int64\n",
      "Targets\n",
      "0    202\n",
      "1    201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify that y_train, y_val and y_test are balanced:\n",
    "print(y_train.value_counts())\n",
    "print(y_val.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247caf38-8a88-4490-8c62-77b3c6537a2b",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca5820d-9606-424d-a991-c4e64c4bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of StandardScaler class:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training data:\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the validation and test sets:\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af79ad21-5fe1-4f22-bf38-6372411ff397",
   "metadata": {},
   "source": [
    "# Data Preprocessing Using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308322db-06ed-4068-a241-6e78f8b5b430",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "We could shuffle using Pandas' `.sample` method. However, it's time to convert the DataFrame to TensorFlow tensors because this is the most robust process, especially for very large datasets. Then, we'll shuffle in TensorFlow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3812e0e1-994b-4047-a2da-4886abe4b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_scaled, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_scaled, dtype=tf.float32)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_scaled, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "400f7f48-b57d-4a0b-a5b4-62874f4bae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([1834, 1834], dtype=int64))\n",
      "(array([0., 1.], dtype=float32), array([201, 202], dtype=int64))\n",
      "(array([0., 1.], dtype=float32), array([202, 201], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Verify that y_train, y_val and y_test are balanced:\n",
    "print(np.unique(y_train_tensor, return_counts=True))\n",
    "print(np.unique(y_val_tensor, return_counts=True))\n",
    "print(np.unique(y_test_tensor, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba140882-83a2-4bff-8cd0-467f6d8afcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to create the tensorflow datasets with a specific batch size:\n",
    "train_set, validation_set, test_set = create_datasets(\n",
    "    x_train_tens=X_train_tensor,  # Tensor of training features\n",
    "    y_train_tens=y_train_tensor,  # Tensor of training labels\n",
    "    x_val_tens=X_val_tensor,  # Tensor of validation features\n",
    "    y_val_tens=y_val_tensor,  # Tensor of validation labels\n",
    "    x_test_tens=X_test_tensor,  # Tensor of test features\n",
    "    y_test_tens=y_test_tensor,  # Tensor of test labels\n",
    "    buffer_size=len(X_train),  # Buffer size for shuffling, set to the length of the training data\n",
    "    batch_size=100  # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb50a0d-082c-4327-9902-de8dc3dc334e",
   "metadata": {},
   "source": [
    "# Baseline Model (with Instructor's Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dff1b-41a5-4c91-a052-82fe4c4ddbeb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "I have named this model a baseline model, even though the parameter values are finely tuned using the same values as the instructor's neural network model. Our goal is to compare my neural network with the instructor's using identical parameters. I also commented out this line of code inside the 'audiobooks_scripts.py' file: 'restore_best_weights=True' because the instructor's early stopping callback is much simpler.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684c70a-1c12-43e3-a38e-ddd76ce0ba43",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "I believe my hands-on approach surpasses the instructor's in terms of code readability and comprehension. Furthermore, I have automated the process more efficiently by passing almost all model parameters, except for the batch size, into a single function (see below). Additionally, my train, validation, and test split works properly by changing only one parameter: the test percentage. Finally, I feed the model with 3 batched and prefetched sets instead of 6, which enhances comprehension.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bbc198-1187-4d8b-b724-4955722e5d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Batch Size</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Runs</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optimization Technique</th>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning Rate</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Momentum</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patience</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Layers Act. Function</th>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Activation Function</th>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epochs</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>List of Hidden Layers</th>\n",
       "      <td>[50, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Accuracy Average</th>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Accuracy Standard Deviation</th>\n",
       "      <td>0.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Loss Average</th>\n",
       "      <td>0.3451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Loss Standard Deviation</th>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Value\n",
       "Batch Size                                                    100\n",
       "Number of Runs                                                 30\n",
       "Optimization Technique                                       adam\n",
       "Loss Function                     sparse_categorical_crossentropy\n",
       "Learning Rate                                               0.001\n",
       "Momentum                                                     None\n",
       "Patience                                                        2\n",
       "Hidden Layers Act. Function                                  relu\n",
       "Output Activation Function                                softmax\n",
       "Epochs                                                        100\n",
       "List of Hidden Layers                                    [50, 50]\n",
       "Test Accuracy Average                                       0.806\n",
       "Test Accuracy Standard Deviation                           0.0054\n",
       "Test Loss Average                                          0.3451\n",
       "Test Loss Standard Deviation                                0.006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df = create_model_train_eval_present_results(\n",
    "    batch_size=100,  # The batch size we used to batched the data in the create_datassets function\n",
    "    optimizer='adam',  # Optimization technique (see function dockstring for the options)\n",
    "    learn_rate=0.001,  # Choosing the default Learning rate for ADAM optimizer\n",
    "    mom=None,  # Momentum parameter for SGD optimizer (ignored if not using 'sgd')\n",
    "    n_range=30,  # Number of training and evaluation cycles to run using the same model\n",
    "    input_size=(X_train.shape[1],),  # Shape of the input features (number of features)\n",
    "    hidden_layer_sizes=[50, 50],  # List of sizes for hidden layers (two hidden layers with 50 neurons each)\n",
    "    activation_fun='relu',  # Activation function for the hidden layers (see function dockstring for the options)\n",
    "    output_size=len(y_train.unique()),  # Number of output units\n",
    "    activation_fun_output='softmax',  # Activation function for the output layer (see function dockstring for the options)\n",
    "    loss_fun='sparse_categorical_crossentropy',  # Loss function for training (see function dockstring for the options)\n",
    "    train_set=train_set,  # Training dataset\n",
    "    patience=2,  # Number of epochs with no improvements on validation loss\n",
    "    epochs=100,  # Number of epochs to train the model\n",
    "    validation_set=validation_set,  # Validation dataset\n",
    "    test_set=test_set,  # Number of epochs with no improvement to stop training\n",
    "    verb=0  # Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch)\n",
    ")\n",
    "\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e4d46-bc72-4828-8dd4-5837b41e980b",
   "metadata": {},
   "source": [
    "# VS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5d7d3-8ba8-4eb4-bc22-fe358369ea95",
   "metadata": {},
   "source": [
    "These are instructor's code results using exactly the same parameters and performing 30 runs using the same model:  \n",
    "***Average Test Accuracy***: 0.8089  \n",
    "***Standard Deviation Test Accuracy***: 0.0162  \n",
    "***Average Test Loss***: 0.3466  \n",
    "***Standard Deviation Test Loss***: 0.0193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741ae4-be0f-4006-bcd8-351eb16f0085",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The results are very close to each other. My approach demonstrates better consistency, with a good reduction in test accuracy and test loss standard deviations. However, my approach produces slightly worse test accuracy and test loss.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdafb53-9332-4c4e-88b7-c91dd38389e5",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35302e45-9def-418d-80a5-de5e3f2e7186",
   "metadata": {},
   "source": [
    "It is very difficult to beat the finely tuned parameters, however we 'll give it a try in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "072eb24e-ac37-467e-97c6-3ba9ae3f02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to create the tensorflow datasets:\n",
    "train_set_2, validation_set_2, test_set_2 = create_datasets(\n",
    "    x_train_tens=X_train_tensor,  # Tensor of training features\n",
    "    y_train_tens=y_train_tensor,  # Tensor of training labels\n",
    "    x_val_tens=X_val_tensor,  # Tensor of validation features\n",
    "    y_val_tens=y_val_tensor,  # Tensor of validation labels\n",
    "    x_test_tens=X_test_tensor,  # Tensor of test features\n",
    "    y_test_tens=y_test_tensor,  # Tensor of test labels\n",
    "    buffer_size=len(X_train),  # Buffer size for shuffling, set to the length of the training data\n",
    "    batch_size=150  # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58ce23c3-ac09-4660-b825-22734e9732b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Batch Size</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Runs</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optimization Technique</th>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning Rate</th>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Momentum</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patience</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Layers Act. Function</th>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Activation Function</th>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epochs</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>List of Hidden Layers</th>\n",
       "      <td>[100, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Accuracy Average</th>\n",
       "      <td>0.8123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Accuracy Standard Deviation</th>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Loss Average</th>\n",
       "      <td>0.3341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Loss Standard Deviation</th>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Value\n",
       "Batch Size                                                    150\n",
       "Number of Runs                                                 30\n",
       "Optimization Technique                                       adam\n",
       "Loss Function                     sparse_categorical_crossentropy\n",
       "Learning Rate                                              0.0003\n",
       "Momentum                                                     None\n",
       "Patience                                                       10\n",
       "Hidden Layers Act. Function                                  relu\n",
       "Output Activation Function                                softmax\n",
       "Epochs                                                        100\n",
       "List of Hidden Layers                                  [100, 100]\n",
       "Test Accuracy Average                                      0.8123\n",
       "Test Accuracy Standard Deviation                           0.0029\n",
       "Test Loss Average                                          0.3341\n",
       "Test Loss Standard Deviation                               0.0031"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_df = create_model_train_eval_present_results(\n",
    "    batch_size=150,\n",
    "    optimizer='adam', \n",
    "    learn_rate=0.0003, \n",
    "    mom=None,  \n",
    "    n_range=30, \n",
    "    input_size=(X_train.shape[1],),  \n",
    "    hidden_layer_sizes=[100, 100],  \n",
    "    activation_fun='relu',  \n",
    "    output_size=len(np.unique(y_train)),  \n",
    "    activation_fun_output='softmax', \n",
    "    loss_fun='sparse_categorical_crossentropy',  \n",
    "    train_set=train_set_2,  \n",
    "    patience=10,  \n",
    "    epochs=100,  \n",
    "    validation_set=validation_set_2,  \n",
    "    test_set=test_set_2, \n",
    "    verb=0\n",
    ")\n",
    "\n",
    "model_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be17ad1-9f04-4340-8e2a-272d7e02ca85",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The results aren't very encouraging. I tried hundreds of different combinations but I didn't manage to increase the model's performance in a very significant way. However, I realized that the model presents robust performance even when the hyperparameters change to very extreme values, such as batch_size=400 or even higher.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa3312-bdca-454b-a08f-56a9e0a62388",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "I noticed that there is a consistent improvement in the results by keeping 'restore_best_weights=True' commented out. This might happen because, by not restoring the best weights, the model continues to learn beyond the point where validation loss stopped improving. This can sometimes allow the model to capture more complex patterns in the data and hence generalize better on unseen data.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
