{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a10799-8487-4779-917a-231cedc56fe1",
   "metadata": {},
   "source": [
    "# Predicting Customer Churn with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140db97e-dabc-48ff-af1a-b33b365743c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.executable  # Display the path to the Python executable ensuring the correct env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c2a54-6ef1-47f9-9647-7bd9b93ef1bf",
   "metadata": {},
   "source": [
    "# Import Libraries & Read the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0f3a5-e1cb-4139-884c-ee81422a0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For numerical operations and arrays.\t\n",
    "import pandas as pd  # For data manipulation and analysis.\t\n",
    "import matplotlib.pyplot as plt  # For basic plotting.\t\n",
    "import tensorflow as tf  # For building and training ML models.\n",
    "from sklearn.preprocessing import StandardScaler  # For creating scaler instances for standardization purposes.\n",
    "from imblearn.under_sampling import RandomUnderSampler  # For reducing the majority class number\n",
    "from sklearn.model_selection import train_test_split \n",
    "from audiobooks_scripts import create_datasets, create_model_train_eval_present_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2b4c9-9f4a-49cd-bde9-2cc079b9cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel file to a DataFrame:\n",
    "df = pd.read_excel('original.xlsx')\n",
    "\n",
    "# Drop customer ID column:\n",
    "df_dropped = df.copy().drop(columns='Customer ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b55325-8085-4662-a502-ff96c66051ea",
   "metadata": {},
   "source": [
    "# Dealing with the Imbalance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed5c1c-4d34-466a-b01d-45c85e655ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the target values are separated:\n",
    "df_dropped['Targets'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98754a-bc89-4721-8981-6770dde6c771",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "From the above code it can be seen that the 15.88% of customers made a purchase again, whereas the rest of the customers didn't. We 'll proceed by undersampling the majority class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88344e91-b586-444b-bb4d-24901aefaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_dropped.drop(columns='Targets')  # Create features\n",
    "y = df_dropped['Targets']  # Create targets\n",
    "\n",
    "# Create an instance of RandomUnderSampler class:\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Undersample the separated data:\n",
    "x_undersampled, y_undersampled = under_sampler.fit_resample(x, y)\n",
    "\n",
    "# Convert to DataFrame:\n",
    "df_undersampled = pd.DataFrame(x_undersampled, columns=x.columns)\n",
    "df_undersampled['Targets'] = y_undersampled\n",
    "\n",
    "# Verify the undersampling:\n",
    "df_undersampled['Targets'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d718b3-f1b9-4103-a8b3-6a90b0f4a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_undersampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32e97e-66da-4546-bb16-a0508bda33e9",
   "metadata": {},
   "source": [
    "# Train, Validation and Test Splits with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38c926-8464-4084-99ee-15f2ff2c4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and the targets from the previous DataFrame:\n",
    "X = df_final.drop(columns='Targets')\n",
    "y = df_final['Targets']\n",
    "\n",
    "# Assign size percentages to variables to automate processes and avoid mistakes:\n",
    "test_perc = 0.09\n",
    "mask_perc = 1 - test_perc\n",
    "val_perc = test_perc / mask_perc\n",
    "\n",
    "# Split into training+validation (mask set) and test sets:\n",
    "X_mask, X_test, y_mask, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=test_perc, \n",
    "    stratify=y,  # Ensure the new set is balanced\n",
    "    random_state=42)\n",
    "\n",
    "# Split the training+validation (mask) set into training and validation sets:\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_mask, \n",
    "    y_mask, \n",
    "    test_size=val_perc, \n",
    "    stratify=y_mask,  # Ensure the new set is balanced\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b968a9-8421-4bf2-bbaf-84e45f49f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that y_train, y_val and y_test are balanced:\n",
    "print(y_train.value_counts())\n",
    "print(y_val.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247caf38-8a88-4490-8c62-77b3c6537a2b",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5820d-9606-424d-a991-c4e64c4bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of StandardScaler class:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training data:\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the validation and test sets:\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af79ad21-5fe1-4f22-bf38-6372411ff397",
   "metadata": {},
   "source": [
    "# Data Preprocessing Using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308322db-06ed-4068-a241-6e78f8b5b430",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "We could shuffle using Pandas' `.sample` method. However, it's time to convert the DataFrame to TensorFlow tensors because this is the most robust process, especially for very large datasets. Then, we'll shuffle in TensorFlow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812e0e1-994b-4047-a2da-4886abe4b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_scaled, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_scaled, dtype=tf.float32)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_scaled, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f7f48-b57d-4a0b-a5b4-62874f4bae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that y_train, y_val and y_test are balanced:\n",
    "print(np.unique(y_train_tensor, return_counts=True))\n",
    "print(np.unique(y_val_tensor, return_counts=True))\n",
    "print(np.unique(y_test_tensor, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba140882-83a2-4bff-8cd0-467f6d8afcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to create the tensorflow datasets with a specific batch size:\n",
    "train_set, validation_set, test_set = create_datasets(\n",
    "    x_train_tens=X_train_tensor,  # Tensor of training features\n",
    "    y_train_tens=y_train_tensor,  # Tensor of training labels\n",
    "    x_val_tens=X_val_tensor,  # Tensor of validation features\n",
    "    y_val_tens=y_val_tensor,  # Tensor of validation labels\n",
    "    x_test_tens=X_test_tensor,  # Tensor of test features\n",
    "    y_test_tens=y_test_tensor,  # Tensor of test labels\n",
    "    buffer_size=len(X_train),  # Buffer size for shuffling, set to the length of the training data\n",
    "    batch_size=100  # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb50a0d-082c-4327-9902-de8dc3dc334e",
   "metadata": {},
   "source": [
    "# Baseline Model (with Instructor's Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dff1b-41a5-4c91-a052-82fe4c4ddbeb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "I have named this model a baseline model, even though the parameter values are finely tuned using the same values as the instructor's neural network model. Our goal is to compare my neural network with the instructor's using identical parameters. I also commented out this line of code inside the 'audiobooks_scripts.py' file: 'restore_best_weights=True' because the instructor's early stopping callback is much simpler.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684c70a-1c12-43e3-a38e-ddd76ce0ba43",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "I believe my hands-on approach surpasses the instructor's in terms of code readability and comprehension. Furthermore, I have automated the process more efficiently by passing almost all model parameters, except for the batch size, into a single function (see below). Additionally, my train, validation, and test split works properly by changing only one parameter: the test percentage. Finally, I feed the model with 3 batched and prefetched sets instead of 6, which enhances comprehension.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbc198-1187-4d8b-b724-4955722e5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = create_model_train_eval_present_results(\n",
    "    batch_size=100,  # The batch size we used to batched the data in the create_datassets function\n",
    "    optimizer='adam',  # Optimization technique (see function dockstring for the options)\n",
    "    learn_rate=0.001,  # Choosing the default Learning rate for ADAM optimizer\n",
    "    mom=None,  # Momentum parameter for SGD optimizer (ignored if not using 'sgd')\n",
    "    n_range=30,  # Number of training and evaluation cycles to run using the same model\n",
    "    input_size=(X_train.shape[1],),  # Shape of the input features (number of features)\n",
    "    hidden_layer_sizes=[50, 50],  # List of sizes for hidden layers (two hidden layers with 50 neurons each)\n",
    "    activation_fun='relu',  # Activation function for the hidden layers (see function dockstring for the options)\n",
    "    output_size=len(y_train.unique()),  # Number of output units\n",
    "    activation_fun_output='softmax',  # Activation function for the output layer (see function dockstring for the options)\n",
    "    loss_fun='sparse_categorical_crossentropy',  # Loss function for training (see function dockstring for the options)\n",
    "    train_set=train_set,  # Training dataset\n",
    "    patience=2,  # Number of epochs with no improvements on validation loss\n",
    "    epochs=100,  # Number of epochs to train the model\n",
    "    validation_set=validation_set,  # Validation dataset\n",
    "    test_set=test_set,  # Number of epochs with no improvement to stop training\n",
    "    verb=0  # Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch)\n",
    ")\n",
    "\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e4d46-bc72-4828-8dd4-5837b41e980b",
   "metadata": {},
   "source": [
    "# VS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5d7d3-8ba8-4eb4-bc22-fe358369ea95",
   "metadata": {},
   "source": [
    "These are instructor's code results using exactly the same parameters and performing 30 runs using the same model:  \n",
    "***Average Test Accuracy***: 0.8089  \n",
    "***Standard Deviation Test Accuracy***: 0.0162  \n",
    "***Average Test Loss***: 0.3466  \n",
    "***Standard Deviation Test Loss***: 0.0193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741ae4-be0f-4006-bcd8-351eb16f0085",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The results are very close to each other. My approach demonstrates better consistency, with a good reduction in test accuracy and test loss standard deviations and a slightly better test loss. However, my approach produces slightly worse test accuracy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdafb53-9332-4c4e-88b7-c91dd38389e5",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35302e45-9def-418d-80a5-de5e3f2e7186",
   "metadata": {},
   "source": [
    "It is very difficult to beat the finely tuned parameters, however we 'll give it a try in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072eb24e-ac37-467e-97c6-3ba9ae3f02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to create the tensorflow datasets:\n",
    "train_set_2, validation_set_2, test_set_2 = create_datasets(\n",
    "    x_train_tens=X_train_tensor,  # Tensor of training features\n",
    "    y_train_tens=y_train_tensor,  # Tensor of training labels\n",
    "    x_val_tens=X_val_tensor,  # Tensor of validation features\n",
    "    y_val_tens=y_val_tensor,  # Tensor of validation labels\n",
    "    x_test_tens=X_test_tensor,  # Tensor of test features\n",
    "    y_test_tens=y_test_tensor,  # Tensor of test labels\n",
    "    buffer_size=len(X_train),  # Buffer size for shuffling, set to the length of the training data\n",
    "    batch_size=150  # Number of samples per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce23c3-ac09-4660-b825-22734e9732b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_df = create_model_train_eval_present_results(\n",
    "    batch_size=150,\n",
    "    optimizer='adam', \n",
    "    learn_rate=0.0003, \n",
    "    mom=None,  \n",
    "    n_range=30, \n",
    "    input_size=(X_train.shape[1],),  \n",
    "    hidden_layer_sizes=[100, 100],  \n",
    "    activation_fun='relu',  \n",
    "    output_size=len(np.unique(y_train)),  \n",
    "    activation_fun_output='softmax', \n",
    "    loss_fun='sparse_categorical_crossentropy',  \n",
    "    train_set=train_set_2,  \n",
    "    patience=10,  \n",
    "    \n",
    "    epochs=100,  \n",
    "    validation_set=validation_set_2,  \n",
    "    test_set=test_set_2, \n",
    "    verb=0\n",
    ")\n",
    "\n",
    "model_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be17ad1-9f04-4340-8e2a-272d7e02ca85",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The results aren't very encouraging. I tried hundreds of different combinations but I didn't manage to increase the model's performance in a very significant way. However, I realized that the model presents robust performance even when the hyperparameters change to very extreme values, such as batch_size=400 or even higher.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa3312-bdca-454b-a08f-56a9e0a62388",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "I noticed that there is a consistent improvement in the results by keeping 'restore_best_weights=True' commented out. This might happen because, by not restoring the best weights, the model continues to learn beyond the point where validation loss stopped improving. This can sometimes allow the model to capture more complex patterns in the data and hence generalize better on unseen data.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
